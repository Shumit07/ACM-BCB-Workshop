ğŸ“Œ Overview

This repository provides a teaching demo and workshop scaffold for exploring Retrieval-Augmented Generation (RAG), lightweight judge evaluation, and agentic extensions for patient-support systems.

The current codebase focuses on:

A Streamlit app with six interactive tabs.

Demonstrations of web scraping, retrieval, and local vs API-based generation.

Logging and evaluation components for reproducibility.

It is not a production system; rather, it is a hands-on learning environment for building toward more autonomous â€œagenticâ€ healthcare AI.

ğŸ—‚ï¸ App Structure (6 Tabs)

ğŸ’¬ HeartWise (Demo)
Live demo of the HeartWise assistant via AnythingLLM API.

ğŸ•¸ï¸ Web Scrape (Example)
Simple, respectful single-page fetch to show how raw context can be ingested.

ğŸ§© Simple RAG (OpenAI)
Reproducible RAG pipeline using OpenAI Chat Completions.

ğŸ§© Simple RAG (Local / Hugging Face)
Offline RAG pipeline running on CPU (MiniLM retrieval + FLAN-T5 generation).

ğŸ”¬ Explore (Lab)
Interactive playground for personas, retrieval depth, and prompt variations.

ğŸ—‚ï¸ Ops & Logs
Lightweight reproducibility: CSV logs for answers and judge scores.

ğŸ§­ HeartWise Workshop System Flow
[START: Instructor launches WorkShop.py]

â”‚
â–¼
[App Boot]
â”‚
â””â”€â”€â”€ 1ï¸âƒ£ Initialize Environment
â€ƒâ€ƒâ”œâ”€â”€ Loads .env for API keys ğŸ”
â€ƒâ€ƒâ”œâ”€â”€ Creates folders: data/, data/logs/, data/scrapes/, prompts/ ğŸ“
â€ƒâ€ƒâ”œâ”€â”€ Writes default heartwise_demo_prompt.txt ğŸ§¾
â€ƒâ€ƒâ””â”€â”€ Creates CSV logs if missing (answers.csv, judge_runs.csv) ğŸª¶

[STREAMLIT UI SETUP]

â”‚
â””â”€â”€â”€ 2ï¸âƒ£ Render Sidebar
â€ƒâ€ƒâ”œâ”€â”€ Shows â€œCrisis Bannerâ€ (Safety Disclaimer) âš ï¸
â€ƒâ€ƒâ”œâ”€â”€ Displays file paths (Data, Logs, Scrapes) ğŸ“‚
â€ƒâ€ƒâ””â”€â”€ Mentions .env usage for keys ğŸ—ï¸

[TAB STRUCTURE INITIALIZED]

â”‚
â””â”€â”€â”€ 3ï¸âƒ£ App Tabs Created
â€ƒâ€ƒ- Tab 1: HeartWise (Demo)
â€ƒâ€ƒ- Tab 2: Web Scrape (Example)
â€ƒâ€ƒ- Tab 3: Simple RAG (OpenAI)
â€ƒâ€ƒ- Tab 4: Simple RAG (Local HF)
â€ƒâ€ƒ- Tab 5: Explore (Teaching Lab)
â€ƒâ€ƒ- Tab 6: Ops & Logs

ğŸ§© TAB 1 â€” HeartWise (Live Demo)

â”‚
â””â”€â”€â”€ 4ï¸âƒ£ When User Clicks â€œAnswer + Judgeâ€
â€ƒâ€ƒâ”œâ”€â”€ Calls _call_anythingllm(question) ğŸŒ
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ Reads API endpoint + token from .env
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ POSTs to AnythingLLM API
â€ƒâ€ƒâ”‚â€ƒâ””â”€â”€ Returns JSON + latency (or error)
â€ƒâ€ƒâ”‚
â€ƒâ€ƒâ”œâ”€â”€ Parses answer + citations (handles many JSON shapes) ğŸ”
â€ƒâ€ƒâ”œâ”€â”€ Renders results on screen
â€ƒâ€ƒâ”œâ”€â”€ Runs Judge-Lite (5 scores: accuracy, safety, empathy, clarity, robustness) ğŸ¯
â€ƒâ€ƒâ””â”€â”€ Logs run â†’ answers.csv and judge_runs.csv ğŸ“Š

âœ… Purpose: Illustrate what a production system looks like, not what weâ€™ll build today.

ğŸŒ TAB 2 â€” Web Scrape (Example)

â”‚
â””â”€â”€â”€ 5ï¸âƒ£ When User Clicks â€œPlan Scrapeâ€ or â€œRun Scrapeâ€
â€ƒâ€ƒâ”œâ”€â”€ Plan Scrape: HEAD request to check page status + size
â€ƒâ€ƒâ”œâ”€â”€ Run Scrape:
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ GET HTML â†’ BeautifulSoup cleans it
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ Removes <script> + <style>
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ Extracts visible text ğŸ§¾
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ Saves Markdown snapshot â†’ data/scrapes/ ğŸ“‘
â€ƒâ€ƒâ”‚â€ƒâ””â”€â”€ Builds scrape_report.csv for reference
â€ƒâ€ƒâ””â”€â”€ Shows results + download button

âœ… Purpose: Demonstrate responsible, single-page data ingestion for later RAG use.

ğŸ§  TAB 3 â€” Simple RAG (OpenAI)

â”‚
â””â”€â”€â”€ 6ï¸âƒ£ Workflow: Cloud RAG Pipeline
â€ƒâ€ƒâ”œâ”€â”€ User enters API key + question
â€ƒâ€ƒâ”œâ”€â”€ load_chunks_parquet() loads local knowledge base ğŸ“š
â€ƒâ€ƒâ”œâ”€â”€ get_minilm_encoder() loads semantic model ğŸ§©
â€ƒâ€ƒâ”œâ”€â”€ retrieve_topk(query, chunks, encoder, k) finds relevant docs ğŸ”
â€ƒâ€ƒâ”‚â€ƒâ””â”€â”€ Displays Top-k evidence table
â€ƒâ€ƒâ”‚
â€ƒâ€ƒâ”œâ”€â”€ Generate Answer:
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ Builds grounded prompt (context + citations)
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ OpenAIAdapter.generate(...) calls OpenAI Chat API
â€ƒâ€ƒâ”‚â€ƒâ””â”€â”€ Returns (answer, citations, latency, prompt)
â€ƒâ€ƒâ”‚
â€ƒâ€ƒâ”œâ”€â”€ Renders answer + citations
â€ƒâ€ƒâ”œâ”€â”€ Runs Judge-Lite scoring ğŸ§®
â€ƒâ€ƒâ””â”€â”€ Logs run â†’ CSVs âœ…

âœ… Purpose: Teach the core RAG pipeline â€” retrieval â†’ prompt â†’ generation â†’ evaluation â†’ logging.

ğŸ’» TAB 4 â€” Simple RAG (Local / Hugging Face)

â”‚
â””â”€â”€â”€ 7ï¸âƒ£ Workflow: Offline RAG Pipeline
â€ƒâ€ƒâ”œâ”€â”€ Choose model (flan-t5-base) + preload âš™ï¸
â€ƒâ€ƒâ”‚â€ƒâ””â”€â”€ LocalHFAdapter.preload() loads model + tokenizer on CPU
â€ƒâ€ƒâ”‚
â€ƒâ€ƒâ”œâ”€â”€ â€œRetrieve Evidenceâ€ â†’ same retrieve_topk() flow
â€ƒâ€ƒâ”œâ”€â”€ â€œGenerate Answer (Local)â€ â†’ LocalHFAdapter.generate(...)
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ Builds prompt with/without citations
â€ƒâ€ƒâ”‚â€ƒâ”œâ”€â”€ Generates text locally (no API)
â€ƒâ€ƒâ”‚â€ƒâ””â”€â”€ Handles retry if output too short
â€ƒâ€ƒâ”‚
â€ƒâ€ƒâ”œâ”€â”€ Displays answer + citations
â€ƒâ€ƒâ”œâ”€â”€ Judge-Lite â†’ evaluate response
â€ƒâ€ƒâ””â”€â”€ Logs run (backend = â€œlocal_hfâ€)

âœ… Purpose: Show offline privacy-friendly version of same RAG loop.

ğŸ§ª TAB 5 â€” Explore (Lab)

â”‚
â””â”€â”€â”€ 8ï¸âƒ£ Interactive Experimentation
â€ƒâ€ƒâ”œâ”€â”€ Lets user switch backend (OpenAI / Local)
â€ƒâ€ƒâ”œâ”€â”€ Edit system prompt live ğŸ§ 
â€ƒâ€ƒâ”œâ”€â”€ Adjust top-k, temperature, max tokens, persona
â€ƒâ€ƒâ”œâ”€â”€ Retrieve â†’ Generate â†’ Judge using chosen backend
â€ƒâ€ƒâ””â”€â”€ Logs every run

âœ… Purpose: Hands-on space to tweak parameters and observe RAG behavior.

ğŸ—‚ï¸ TAB 6 â€” Ops & Logs

â”‚
â””â”€â”€â”€ 9ï¸âƒ£ After Session
â€ƒâ€ƒâ”œâ”€â”€ Reads last 20 runs from answers.csv, judge_runs.csv ğŸ“Š
â€ƒâ€ƒâ”œâ”€â”€ Displays + allows downloads
â€ƒâ€ƒâ””â”€â”€ Shows config snapshot (model, paths, defaults)

âœ… Purpose: Reinforce reproducibility & transparency.

[END: User completes workshop]

ğŸ§­ What students will actually use
Shared Core (used by both paths)

Concept loop: Retrieve â†’ Ground â†’ Generate â†’ Evaluate â†’ Log

Code pieces they touch:

Load knowledge: load_chunks_parquet() â†’ returns List[Chunk]

Encoder: get_minilm_encoder() (MiniLM)

Retrieval: retrieve_topk(query, chunks, encoder, k)

Prompt: build_flan_prompt(question, selected_chunks, system_text, persona)

Judge: judge_lite(question, answer, citations)

Logging: log_answer_row(...) and log_judge_row(...)
These are all wired into Tab 3 (OpenAI) and Tab 4 (Local). 

PATH A â€” OpenAI API RAG (Tab 3)

[START: Student on Tab 3]
â”‚
â””â”€â”€ 1) Provide OPENAI_API_KEY (UI field) ğŸ”
    - Stored in session/env for this run
â”‚
â””â”€â”€ 2) Retrieve Evidence ğŸ”
    - chunks = load_chunks_parquet()
    - encoder = get_minilm_encoder()
    - (df_topk, selected) = retrieve_topk(question, chunks, encoder, k)
â”‚
â””â”€â”€ 3) Build Prompt ğŸ§©
    - system_text = read_demo_prompt()
    - prompt = build_flan_prompt(question, selected, system_text, persona)
â”‚
â””â”€â”€ 4) Generate with OpenAI â˜ï¸
    - adapter = OpenAIAdapter(model="gpt-4o-mini")
    - (answer, citations, latency, prompt_used) = adapter.generate(...)
â”‚
â””â”€â”€ 5) Evaluate + Log ğŸ“Š
    - scores = judge_lite(question, answer, citations)
    - log_answer_row(... "openai", ...)
    - log_judge_row(... "openai", ...)
â”‚
â–¼
[END: Answer shown + citations + scores]

PATH B â€” Local HF on CPU (Tab 4)

[START: Student on Tab 4]
â”‚
â””â”€â”€ 1) Choose model (default: flan-t5-base) ğŸ§±
    - Optional: click â€œPreload Modelsâ€ (CPU-friendly load)
â”‚
â””â”€â”€ 2) Retrieve Evidence ğŸ”
    - Same as Path A:
      chunks = load_chunks_parquet()
      encoder = get_minilm_encoder()
      (df_topk, selected) = retrieve_topk(...)
â”‚
â””â”€â”€ 3) Build Prompt ğŸ§©
    - (done inside adapter.generate; same structure/intent)
â”‚
â””â”€â”€ 4) Generate Locally ğŸ–¥ï¸
    - adapterL = LocalHFAdapter("google/flan-t5-base")
    - adapterL.preload() if needed
    - (answer, citations, latency, prompt_used) = adapterL.generate(...)
      â€¢ trims context
      â€¢ first pass requires citations; if too short, retries w/o markers
â”‚
â””â”€â”€ 5) Evaluate + Log ğŸ“Š
    - scores = judge_lite(...)
    - log_answer_row(... "local_hf", ...)
    - log_judge_row(... "local_hf", ...)
â”‚
â–¼
[END: Answer (Local) + citations + scores]



â€ƒâ€ƒâ–¼
âœ… Outcome Summary

Students understand each stage of the RAG loop:
Retrieve â†’ Ground â†’ Generate â†’ Evaluate â†’ Log

They can run both cloud and local versions.

Theyâ€™ve seen a small, safe scraping example.

They know how the app self-initializes and manages state.
