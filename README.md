ğŸ’¡ HeartWise Simple-RAG Workshop
Educational Demonstration of Retrieval-Augmented Generation (RAG)

The HeartWise Simple-RAG Workshop introduces attendees to the core principles of Retrieval-Augmented Generation â€” the architecture behind explainable, context-aware large language models.
Rather than focusing on agentic automation, this version isolates the foundation: how LLMs retrieve, ground, and generate safe, evidence-based answers.

The workshop demonstrates two independent but parallel systems:

Path A â€” Cloud RAG (OpenAI): Uses the OpenAI API for inference.

Path B â€” Local RAG (CPU / GPU): Runs Hugging Face models locally or through a Colab GPU endpoint for those without API access.

Together, these pipelines form the educational backbone of the full HeartWise Agentic system â€” showing how retrieval, grounding, and reasoning interact before multi-agent layers are added.

ğŸ§  Conceptual Overview

The Simple-RAG framework replicates the cognitive workflow of a responsible AI assistant:

Context Retrieval:
The system identifies and extracts the most relevant text segments (â€œchunksâ€) from a small curated dataset on heart-failure self-care.

Grounded Prompt Construction:
Retrieved evidence is formatted into a concise context window, ensuring the modelâ€™s output remains fact-anchored.

Controlled Generation:
The LLM (either OpenAI GPT or FLAN-T5) answers strictly within the retrieved context.
It cites sources inline ([1], [2], â€¦) and refuses to speculate.

Transparent Evaluation:
Responses can be optionally scored using a heuristic â€œJudge-Liteâ€ rubric (accuracy, safety, empathy, clarity, robustness).

## âš™ï¸ System Architecture

The diagram below shows the dual-path RAG workflow used in the workshop:


<img width="606" height="190" alt="image" src="https://github.com/user-attachments/assets/3fe8848d-1682-463c-ac66-122b72d1edd8" />


ğŸ§© Dual-Path Demonstration
â˜ï¸ Path A â€” OpenAI RAG

For attendees with an OpenAI API Key

Retrieval â†’ Prompt â†’ Generation loop using gpt-4o-mini.

Demonstrates high-quality cloud inference with reproducible logging.

Focus: API integration, latency awareness, citation control.

ğŸ’» Path B â€” Local RAG (CPU / GPU / Colab)

For attendees without an API key

Uses flan-t5-base (or flan-t5-large) via Hugging Face Transformers.

Can optionally connect to a Colab GPU endpoint (RemoteHFAdapter).

Focus: privacy, transparency, model introspection.

â˜ï¸ Running the GPU (Colab) Version

If your local computer doesnâ€™t have enough CPU power to run larger models, you can offload generation to a Colab GPU using ngrok and FastAPI.
This allows you to compare CPU vs GPU performance live during the workshop.

ğŸ§© 1ï¸âƒ£ Open a New Google Colab Notebook

Visit Google Colab
 â†’ click New Notebook

Copy the Colab code block from this repository (e.g., colab_server.py)

Paste it into a new Colab cell

âš™ï¸ 2ï¸âƒ£ Install Dependencies

Run the following cell first:

!pip -q install fastapi uvicorn transformers accelerate safetensors pyngrok --upgrade

ğŸ” 3ï¸âƒ£ Create a Free ngrok Account

Go to https://dashboard.ngrok.com/signup

After verifying your email, click
Getting Started â†’ Your Authtoken

Copy your personal token â€” it looks like this:

2oxvxxxxxxxxxxxxxxxxxxxxxx_QwPxxxxxxxxxxxxxxxxxxxxx

ğŸª„ 4ï¸âƒ£ Add the ngrok Token in Colab

In a new cell before running the FastAPI server, paste:

from pyngrok import ngrok
ngrok.set_auth_token("YOUR_AUTHTOKEN_HERE")

ğŸš€ 5ï¸âƒ£ Start the FastAPI Server

Run the main Colab block:

!python colab_server.py


You should see output similar to:

ğŸŒ Public URL: https://abcd1234.ngrok-free.app


Copy this public URL â€” itâ€™s your live GPU endpoint.

ğŸ’» 6ï¸âƒ£ Connect Streamlit â†’ Colab GPU

In your local app_local.py (Tab 4 of the app):

Paste the copied ngrok URL into the Colab URL input box

Choose the GPU option in the interface

Run your query â€” it will now use the remote GPU model

ğŸ“ Teaching Objectives

By the end of the workshop, participants will:

Understand how retrieval reduces hallucination risk.

Compare cloud vs local model behavior on identical prompts.

Inspect how context size, temperature, and top-k affect factual grounding.

Recognize how this modular RAG block feeds into more advanced agentic systems.

ğŸ§© Example Prompts

â€œWhat daily habits help reduce heart-failure hospitalizations?â€

â€œWhich symptoms indicate fluid retention in heart-failure patients?â€

â€œWhen should a person with heart failure seek emergency care?â€

âš–ï¸ Ethical Framing

HeartWise Simple-RAG is an educational prototype.
It does not provide medical advice, diagnosis, or treatment.
Its purpose is to teach responsible AI design, transparency, and data stewardship within health-education contexts.

| Component                | Purpose                                      |
| ------------------------ | -------------------------------------------- |
| **Retriever**            | Encodes and ranks context snippets (MiniLM)  |
| **Generator**            | Produces grounded answer (OpenAI / FLAN-T5)  |
| **Evaluator (Optional)** | Heuristically scores answer quality          |
| **Interface**            | Streamlit UI for reproducible teaching demos |

